{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName('core').getOrCreate()\n",
    "path = \"./statesCSV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "df=spark.read.csv(path+\"/totalStates.csv\",header=True,inferSchema=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.filter(df[\"carbon_intensity\"]>200).show()\n",
    "\n",
    "max = df.agg({\"carbon_intensity\": \"max\"}).collect()[0][0]\n",
    "min = df.agg({\"carbon_intensity\": \"min\"}).collect()[0][0]\n",
    "\n",
    "df.describe().show()\n",
    "df.filter(df[\"carbon_intensity\"]==min).select(\"stato\").distinct().show()\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import time\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "def get_fascia_oraria(x):\n",
    "    print(x)\n",
    "    return x+\"a\"\n",
    "\n",
    "df = spark.read.csv(path + \"/totalStates.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df=df.withColumn(\"fascia_oraria\",fascia_oraria(df[\"timestamp\"]))\n",
    "df2=df.select([unix_timestamp((\"timestamp\"), \"HH:mm dd-MM-yyyy\").alias(\"timestamp_inMillis\")])\n",
    "df2.show()\n",
    "#df3=df\n",
    "df3=df2.join(df)\n",
    "df3.show(300)\n",
    "\n",
    "print(min)\n",
    "print(max)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import multiprocessing\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "n_core = multiprocessing.cpu_count()\n",
    "path = \"./statesCSV\"\n",
    "\n",
    "precedent_dates_filters=None\n",
    "new_date_filter=None\n",
    "\n",
    "stato_maggiore = udf(lambda x: get_stato_maggiore(x), StringType())\n",
    "fascia_oraria = udf(lambda x: get_fascia_oraria(x), StringType())\n",
    "map_consumo = udf(lambda x, y, z: get_consumo(x, y, z), FloatType())\n",
    "sum_import_export=udf(lambda x: get_sum_import_export(x), FloatType())\n",
    "repair_total_production=udf(lambda x, y: get_new_total_production(x, y), FloatType())\n",
    "\n",
    "\n",
    "col_static = ['timestamp_inMillis', 'timestamp' , 'carbon_intensity' , 'low_emissions' , 'renewable_emissions',\n",
    "              'total_production', 'total_emissions', 'exchange_export', 'exchange_import', 'stato', 'consumo',\n",
    "              'fascia_oraria']\n",
    "\n",
    "col_classic = ['timestamp','fascia_oraria','stato_maggiore','stato','carbon_intensity','avg(carbon_intensity)','low_emissions','renewable_emissions',\n",
    "               'total_production','total_emissions','consumo','nucleare_installed_capacity','nucleare_production','nucleare_emissions',\n",
    "               'geotermico_installed_capacity','geotermico_production','geotermico_emissions','biomassa_installed_capacity','biomassa_production',\n",
    "               'biomassa_emissions','carbone_installed_capacity','carbone_production','carbone_emissions','eolico_installed_capacity','eolico_production',\n",
    "               'eolico_emissions','fotovoltaico_installed_capacity','fotovoltaico_production','fotovoltaico_emissions','idroelettrico_installed_capacity',\n",
    "               'idroelettrico_production','idroelettrico_emissions','accumuloidro_installed_capacity','accumuloidro_production','accumuloidro_emissions',\n",
    "               'batterieaccu_installed_capacity','batterieaccu_production','batterieaccu_emissions','gas_installed_capacity','gas_production',\n",
    "               'gas_emissions','petrolio_installed_capacity','petrolio_production','petrolio_emissions','sconosciuto_installed_capacity',\n",
    "               'sconosciuto_production','sconosciuto_emissions','exchange_export','sum_export','exchange_import','sum_import']\n",
    "\n",
    "col_pro =       ['sum(total_production)','sum(total_emissions)','sum(nucleare_installed_capacity)',\n",
    "                 'sum(nucleare_production)','sum(nucleare_emissions)','sum(geotermico_installed_capacity)','sum(geotermico_production)',\n",
    "                 'sum(geotermico_emissions)','sum(biomassa_installed_capacity)','sum(biomassa_production)','sum(biomassa_emissions)',\n",
    "                 'sum(carbone_installed_capacity)','sum(carbone_production)','sum(carbone_emissions)','sum(eolico_installed_capacity)',\n",
    "                 'sum(eolico_production)','sum(eolico_emissions)','sum(fotovoltaico_installed_capacity)','sum(fotovoltaico_production)',\n",
    "                 'sum(fotovoltaico_emissions)','sum(idroelettrico_installed_capacity)','sum(idroelettrico_production)','sum(idroelettrico_emissions)',\n",
    "                 'sum(accumuloidro_installed_capacity)','sum(accumuloidro_production)','sum(accumuloidro_emissions)','sum(batterieaccu_installed_capacity)',\n",
    "                 'sum(batterieaccu_production)','sum(batterieaccu_emissions)','sum(gas_installed_capacity)','sum(gas_production)','sum(gas_emissions)',\n",
    "                 'sum(petrolio_installed_capacity)','sum(petrolio_production)','sum(petrolio_emissions)','sum(sconosciuto_installed_capacity)',\n",
    "                 'sum(sconosciuto_production)','sum(sconosciuto_emissions)','sum(consumo)','sum(sum_import)','sum(sum_export)']\n",
    "\n",
    "col_union=[]\n",
    "for i in col_classic:\n",
    "    col_union.append(i)\n",
    "    for j in col_pro:\n",
    "        if(j.find(i) != -1):\n",
    "            #col_union.append(j)\n",
    "            break\n",
    "\n",
    "\n",
    "def get_sum_import_export(x):\n",
    "    sum= 0\n",
    "    try :\n",
    "        n = x.split(\"@\")\n",
    "        for i in n:\n",
    "            if (i):\n",
    "                try :\n",
    "                    value = i.split(\"_\")[2]\n",
    "                    if (value == \"nan\"):\n",
    "                        value = 0\n",
    "                    sum += float(value)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    sum += 0\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        sum += 0\n",
    "    return sum\n",
    "\n",
    "\n",
    "def get_stato_maggiore(x):\n",
    "    try:\n",
    "        return x.split(\"(\")[1].replace(\")\", \"\")\n",
    "    except:\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_fascia_oraria(x):\n",
    "    hh = x.split(\":\")[0]\n",
    "    hh = int(hh)\n",
    "    if (hh >= 00 and hh < 6):\n",
    "        return \"notte\"\n",
    "    elif (hh >= 6 and hh < 12):\n",
    "        return \"mattina\"\n",
    "    elif (hh >= 12 and hh < 18):\n",
    "        return \"pomeriggio\"\n",
    "    elif (hh >= 18 and hh <= 23):\n",
    "        return \"sera\"\n",
    "\n",
    "\n",
    "def get_consumo(x,y,z):\n",
    "\n",
    "    import_q = 0\n",
    "    export_q = 0\n",
    "    try:\n",
    "        n=y.split(\"@\")\n",
    "        for i in n:\n",
    "            if (i):\n",
    "                try:\n",
    "                    value = i.split(\"_\")[2]\n",
    "                    if (value == \"nan\"):\n",
    "                        value = 0\n",
    "                    import_q+= float(value)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    import_q += 0\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        import_q += 0\n",
    "\n",
    "    try :\n",
    "        n = z.split(\"@\")\n",
    "        for i in n:\n",
    "            if (i):\n",
    "                try :\n",
    "                    value=i.split(\"_\")[2]\n",
    "                    if(value==\"nan\"):\n",
    "                        value=0\n",
    "                    export_q += float(value)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    export_q += 0\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        export_q += 0\n",
    "\n",
    "    cont = float(x) + import_q + export_q\n",
    "    return cont\n",
    "def get_new_total_production(x,y):\n",
    "    import_q = 0\n",
    "\n",
    "    try:\n",
    "        n = y.split(\"@\")\n",
    "        for i in n:\n",
    "            if (i):\n",
    "                try:\n",
    "                    value=i.split(\"_\")[2]\n",
    "                    if(value==\"nan\"):\n",
    "                        value=0\n",
    "                    import_q += float(value)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    import_q += 0\n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        import_q += 0\n",
    "    return float(x)-import_q\n",
    "\n",
    "\n",
    "#giorni>fasciaoraria>stati/sottostati>fonti\n",
    "# if(filtrifasciaoraria==full):\n",
    "#     fasciaoraria=giorni\n",
    "# else:\n",
    "#     fasciaoraria=filt(filtrifasciaoraria(giorni))\n",
    "#\n",
    "\n",
    "\n",
    "# if(precedent_dates_filters!=new_date_filter):\n",
    "#     pass\n",
    "#     #recalculate date database\n",
    "#\n",
    "# def filter_on_dates(dates):\n",
    "#     pass\n",
    "#     #return new database\n",
    "# def get_nstates_on_source(n,source_list):\n",
    "#     pass\n",
    "#\n",
    "millis_day=86400\n",
    "fasce_MPSN=['mattina','pomeriggio','sera','notte']\n",
    "\n",
    "def query_timestamp(df, giorni):\n",
    "    tmp=None\n",
    "    for i in giorni:\n",
    "        if(tmp):\n",
    "            tmp=tmp.union(df.filter(df['timestamp_inSeconds']>=i).filter(df['timestamp_inSeconds']<i+millis_day))\n",
    "        else:\n",
    "            tmp=df.filter(df['timestamp_inSeconds']>=i).filter(df['timestamp_inSeconds']<i+millis_day)\n",
    "    return tmp\n",
    "\n",
    "def query_fascia_oraria(df, fasce):\n",
    "    fasce_tmp = []\n",
    "    for s in fasce:\n",
    "        fasce_tmp.append('fascia_oraria=\"' + s + '\"')\n",
    "\n",
    "    return df.filter(\" or \".join(fasce_tmp))\n",
    "\n",
    "def query_stati(df, stati):\n",
    "    stati_tmp=[]\n",
    "    for s in stati:\n",
    "        stati_tmp.append('stato=\"'+s+'\"')\n",
    "\n",
    "    return df.filter(\" or \".join(stati_tmp))\n",
    "\n",
    "\n",
    "def query_fonte(df, fonti):\n",
    "    col_selezionate = col_static\n",
    "    for f in fonti :\n",
    "        col_selezionate.append(f + \"_installed_capacity\")\n",
    "        col_selezionate.append(f + \"_production\")\n",
    "        col_selezionate.append(f + \"_emissions\")\n",
    "\n",
    "    return df.select(*col_selezionate)\n",
    "\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName('Core').getOrCreate()\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function JavaObject.__init__.<locals>.<lambda> at 0x000001C7E0CCD0D0>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\", line 1341, in <lambda>\n",
      "  File \"C:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\", line 645, in _garbage_collect_object\n",
      "  File \"C:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\", line 964, in garbage_collect_object\n",
      "  File \"C:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\", line 1031, in send_command\n",
      "  File \"C:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\", line 979, in _get_connection\n",
      "  File \"C:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\", line 985, in _create_connection\n",
      "  File \"C:\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\", line 1115, in start\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "    #print(spark.getActiveSession())\n",
    "\n",
    "\n",
    "    '''\n",
    "    path = \"./states\"\n",
    "\n",
    "    print(os.listdir(path))\n",
    "\n",
    "    for f in os.listdir(path):\n",
    "        print(f)\n",
    "        xcel = pd.read_excel(path + \"/\" + f)\n",
    "        f=f.split(\".\")[0]\n",
    "        xcel[\"stato\"]=f\n",
    "\n",
    "        xcel.to_csv(\"./statesCSV/\"+ f +\".csv\", index=False)\n",
    "\n",
    "    df=0\n",
    "    count=0\n",
    "    path1=\"./statesCSV/\"\n",
    "    print(os.listdir(path1))\n",
    "    for f in os.listdir(path1):\n",
    "        if(count == 0):\n",
    "            df=pd.read_csv(path1 + f)\n",
    "            count = 1\n",
    "        else:\n",
    "            df=pd.concat([df,pd.read_csv(path1 + f)])\n",
    "    df.to_csv(path1 + \"totalStates\" + \".csv\", index=False)\n",
    "    '''\n",
    "    df = spark.read.csv(path + \"/totalstates.csv\", header=True, inferSchema=True)\n",
    "\n",
    "    df = df.withColumn(\"stato_maggiore\", stato_maggiore(df[\"stato\"]))\n",
    "    df = df.withColumn(\"total_production\", repair_total_production(df['total_production'], df['exchange_import']))\n",
    "\n",
    "    averaged = df.select('timestamp', 'stato_maggiore', 'carbon_intensity').groupBy('timestamp', 'stato_maggiore').avg()\n",
    "    df = df.join(averaged,\n",
    "                  (df['timestamp'] == averaged['timestamp']) & (df['stato_maggiore'] == averaged['stato_maggiore']),\n",
    "                  \"inner\").drop(df.timestamp).drop(df.stato_maggiore)\n",
    "\n",
    "    df = df.withColumn(\"fascia_oraria\", fascia_oraria(df[\"timestamp\"]))\n",
    "\n",
    "    df = df.withColumn(\"consumo\", map_consumo(df['total_production'], df['exchange_import'], df['exchange_export']))\n",
    "\n",
    "    df = df.withColumn(\"sum_import\", sum_import_export(df['exchange_import']))\n",
    "\n",
    "    df = df.withColumn(\"sum_export\", sum_import_export(df['exchange_export']))\n",
    "    #df.filter(df['timestamp'] == \"19:00 20-04-2022\").filter(df['stato_maggiore'] == \"Italia\").show()\n",
    "\n",
    "\n",
    "    df = df.select([unix_timestamp((\"timestamp\"), \"HH:mm dd-MM-yyyy\").alias(\"timestamp_inSeconds\"),*col_union])\n",
    "\n",
    "    print(\"siamo qua 1\")\n",
    "    start = time.time()\n",
    "    df1 = df.cache()\n",
    "    df1.count()\n",
    "    print(\"Tempo di cache = \",time.time() - start)\n",
    "\n",
    "    start = time.time()\n",
    "    sum1= df1.select('stato','stato_maggiore','total_production').groupBy('stato','stato_maggiore').avg().groupBy('stato_maggiore').sum().sort(col('sum(avg(total_production))').desc())\n",
    "    sum1.show()\n",
    "\n",
    "    sum1 = df1.select('stato','total_production').groupBy('stato').avg().sort(col('avg(total_production)').desc())\n",
    "    sum1.show()\n",
    "\n",
    "    #query sul carbon_intensity stato intero\n",
    "    sum1 = df1.select('stato_maggiore', 'carbon_intensity').groupBy('stato_maggiore').avg().sort(col('avg(carbon_intensity)').desc())\n",
    "    sum1.show()\n",
    "    # query sul carbon_intensity stato/sottostati\n",
    "    sum1 = df1.select('stato', 'carbon_intensity').groupBy('stato').avg().sort(col('avg(carbon_intensity)').desc())\n",
    "    sum1.show()\n",
    "\n",
    "    sum1 = df1.select('time_stamp','fotovoltaico_production').groupBy('time_stamp').avg()\n",
    "    sum1.show()\n",
    "    print(\"Tempo = \", time.time() - start)\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    df1.show()\n",
    "    print(df1.count())\n",
    "\n",
    "    print(\"siamo qua 2\")\n",
    "    df1.filter(df1['timestamp'] == \"19:00 20-04-2022\").filter(df1['stato_maggiore'] == \"Italia\").select(\"sum(sum_import)\").show()\n",
    "    df1.filter(df1['timestamp'] == \"19:00 20-04-2022\").filter(df1['stato_maggiore'] == \"Italia\").select(\"sum(sum_export)\").show()\n",
    "    print(\"siamo qua 3\")\n",
    "    #print(df1.describe())\n",
    "    #df1.show()\n",
    "    #df.show(300)\n",
    "\n",
    "    #print(df.count())\n",
    "\n",
    "    print(df1.filter(df1['carbon_intensity']>300).count())\n",
    "    print(df1.filter(df1['carbon_intensity']<200).count())\n",
    "\n",
    "    x=[1650492000,1650578400]\n",
    "    df1=query_timestamp(df,x)\n",
    "\n",
    "    y=['mattina','pomeriggio','sera','notte']\n",
    "    df1=query_fascia_oraria(df1,y)\n",
    "\n",
    "\n",
    "    stati=[\"Austria\",\"Francia\",\"Danimarca orientale (Danimarca)\"]\n",
    "\n",
    "    df1=query_stati(df1,stati).select('stato').distinct().show()\n",
    "    fonti=['nucleare','geotermico']\n",
    "\n",
    "    time.sleep(10000)\n",
    "    df1=query_fonte(df,fonti)\n",
    "    df1.show(300)\n",
    "\n",
    "    df1.filter(df['stato_maggiore']=='Italia').dropDuplicates((['stato'])).show(300)\n",
    "\n",
    "    #print(\"...\",df.filter(df['timestamp_inMillis'] >= x).filter(df['timestamp_inMillis'] <= y).count())\n",
    "    #df.filter(df['timestamp_inMillis'] >= x).filter(df['timestamp_inMillis'] <= y).show()\n",
    "    #df.show(300)\n",
    "\n",
    "    #time.sleep(10000)\n",
    "    print(\"FINE\")\n",
    "    '''\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}